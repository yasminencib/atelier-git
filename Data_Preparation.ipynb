{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasminencib/atelier-git/blob/master/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-65vpVg40T"
      },
      "source": [
        "#### <font color='blue'> <h1 align=\"center\"> Notebook 1 : Data Prepation <br> 4DS_2022/2023 <h1> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y0ZetevyE1g"
      },
      "source": [
        "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
        "I. Pandas</p><br>\n",
        "pandas is a Python library for data analysis. It offers a number of data exploration, cleaning, and transformation operations that are essential for working with data in Python.\n",
        "\n",
        "pandas relies on numpy and scipy providing easy-to-use data structures and data manipulation functions with built-in indexing.\n",
        "\n",
        "  The main features of pandas are:\n",
        "* Generation of descriptive statistics on the data\n",
        "* Data cleaning using built-in pandas functions\n",
        "* Frequent data operations for data subset, filtering, insertion, deletion and aggregation\n",
        "* Merging multiple datasets using dataframes\n",
        "\n",
        "**Resources:**\n",
        "* *pandas* Documentation: http://pandas.pydata.org/pandas-docs/stable/\n",
        "* *Python for Data Analysis* by Wes McKinney\n",
        "* *Python Data Science Handbook* by Jake VanderPlas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVkPPCJ6yE1g"
      },
      "source": [
        "Warning: `pandas` is a library that evolves regularly, so we recommend that you use at least `pandas` in its version 1.0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8wVzhS1yE1g",
        "outputId": "91c8385a-cf32-4e5b-b609-7b629517c79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy version 1.26.4\n",
            "pandas version 2.1.4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "#Pour afficher la version de  numpy\n",
        "print(f\"numpy version {np.__version__}\")\n",
        "\n",
        "import pandas as pd\n",
        "# Pour afficher la version de pandas\n",
        "print(f\"pandas version {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N816rgPij6Pu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import isnan\n",
        "from pandas import read_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAD9azFyE1h"
      },
      "source": [
        "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
        "II. Create a Dataframe</p><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zxBKqD7yE1h"
      },
      "source": [
        "#### Data structures in `pandas`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jIT8msdyE1h"
      },
      "source": [
        "`Pandas` offers us two main data structures namely: the `Series` class and the `DataFrame` class.\n",
        "\n",
        "1. A `Series` is a one-dimensional array where each element is indexed (often strings).\n",
        "2. A `DataFrame` is a two-dimensional array where rows and columns are indexed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNSiEtQnUhN"
      },
      "source": [
        "\n",
        "## Method (1): Create a Dataframe from multiple series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv6ObGMUo_K3"
      },
      "source": [
        "We start by creating a Pandas series which is like a column in a table with index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLJd_U2YnTQx",
        "outputId": "f2e5912a-a351-459d-9b54-5f204759fbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a series to list students with their age\n",
        "#age = pd.Series([23, 22, 25], index=['Student1', 'Student2', 'Student3'])\n",
        "#print(age)\n",
        "print(\"===================================================\")\n",
        "# Create a series to list students with their high\n",
        "#height = pd.Series([155, 175, 168], index=['Student1', 'Student3', 'Student4'])\n",
        "#print(height)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4P1fYHfqIst"
      },
      "source": [
        "**A dataframe** can be composed of multiple series, or it can be defined as a collection of series that can be used to analyze data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNSV621xpvsV"
      },
      "outputs": [],
      "source": [
        "#stat = pd.DataFrame({'age': age, 'height': height})\n",
        "#stat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl9Igzq2rFwz"
      },
      "source": [
        "## Method (2): Load a Dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL3aiWJurE0C"
      },
      "outputs": [],
      "source": [
        "## load dataset from a url\n",
        "#url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
        "# help to predict whether or not a horse can survive based upon past medical conditions\n",
        "#stat2 = read_csv(url, header=None, na_values='?')\n",
        "#print(stat2.shape)\n",
        "#stat2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gIzW9pzskR7"
      },
      "outputs": [],
      "source": [
        "## load dataset from a file on JupyterLab\n",
        "#stat3=pd.read_excel('data.xlsx', header=None, na_values='?')\n",
        "# print(stat3.shape)\n",
        "# stat3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1q8bdAPuAGP"
      },
      "source": [
        "## Method(3): Create a Dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6gcd-GQuGVd"
      },
      "outputs": [],
      "source": [
        "# x=np.nan\n",
        "# df=pd.DataFrame(\n",
        "# {   \"Type\": ['Student1', 'Student2', 'Student3', 'Student4' ],\n",
        "\n",
        "#  \"order\": [2,10, x, 10],\n",
        "#  \"age\": [23,x , 25, 0],\n",
        "#  \"height\": [155, 160, x, 160],\n",
        "# }\n",
        "# )\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pe59EMoyE1j"
      },
      "source": [
        "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
        "III.Get information about a dataset</p><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYXqqqwmyE1j"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv('titanic.csv')\n",
        "# titanic data set help to predicts which passengers survived the Titanic shipwreck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXSzp39KyE1j"
      },
      "outputs": [],
      "source": [
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L3qosfSyE1j"
      },
      "outputs": [],
      "source": [
        "# data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmRic6bcyE1j"
      },
      "outputs": [],
      "source": [
        "# data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuESzY-qyE1j"
      },
      "outputs": [],
      "source": [
        "# data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jlgha_0yE1j"
      },
      "outputs": [],
      "source": [
        "# data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uygNA3BvyE1j"
      },
      "outputs": [],
      "source": [
        "# data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0lpmEYSueMT"
      },
      "source": [
        "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
        "IV Data Preparation </p><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzippjRyE1k"
      },
      "source": [
        "<font color='blue' ><h3>\n",
        "IV.1. Data Cleaning </p><h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-0u9PH0yE1k"
      },
      "source": [
        "## (1): Checking for NAN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9bTYOIjv1zD"
      },
      "outputs": [],
      "source": [
        "# df.isnull().values.any()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezxJfHJNv8PB"
      },
      "outputs": [],
      "source": [
        "# df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwLzf-xLwJBE"
      },
      "outputs": [],
      "source": [
        "# df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywWmZbIswNgc"
      },
      "outputs": [],
      "source": [
        "# df.notnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN47G33rv-06"
      },
      "outputs": [],
      "source": [
        "# df.notnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2bGmVKSw6pE"
      },
      "outputs": [],
      "source": [
        "# df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LECSrNiIxbQR"
      },
      "outputs": [],
      "source": [
        "# df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eyo3m6Ixcii"
      },
      "outputs": [],
      "source": [
        "# df.notna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtJLyZw0xvqd"
      },
      "source": [
        "## (2): Common Methods to Imputing Missing Data\n",
        "\n",
        "There are different methods that we use to impute missing data such that: Mean strategy, Median technique and Mode technique.\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEM3dRBUgHrA"
      },
      "source": [
        "#### *) Mean strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joWyOcmux4jR"
      },
      "outputs": [],
      "source": [
        "#One of the technique is mean imputation in which the missing values are replaced with the mean value of the entire feature column\n",
        "# df.fillna(df.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAob9nqfyNgE"
      },
      "source": [
        "#### *) Median technique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg_ELTM0yKle"
      },
      "outputs": [],
      "source": [
        " #Median imputation in which the missing values are replaced with the median value of the entire feature column.\n",
        "#When the data is skewed, it is good to consider using median value for replacing the missing values.\n",
        "\n",
        "# df.fillna(df.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZrTvduvyjxD"
      },
      "source": [
        "#### *) Mode technique (most frequent value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ebiRzTQyrkh"
      },
      "outputs": [],
      "source": [
        "\n",
        "#mode imputation in which the missing values are replaced with the mode value or most frequent value of the entire feature column. When the data is skewed, it is good to consider using mode value for replacing the missing values. For data points such as salary field,\n",
        "#you may consider using mode for replacing the values\n",
        "# df['height'] = df['height'].fillna(df['height'].mode()[0])\n",
        "# df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8CIcnx1yul6"
      },
      "outputs": [],
      "source": [
        "# df['order'] = df['order'].fillna(df['order'].mode()[0])\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTs6zyQvyE1p"
      },
      "source": [
        "<font color='blue' ><h3>\n",
        "IV.2. Feature selection </p><h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRsXwNo72uEn"
      },
      "source": [
        "Feature selection is used to determine the best set of features for building useful models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCyb71LXyfmi"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "# The objective of the dataset is to diagnostically predict whether or not a patient has diabetes,\n",
        "# dataframe = pd.read_csv(url, names=names)\n",
        "# dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqbiHEHlAwU"
      },
      "source": [
        "After loading the dataframe, we convert it to a NumPy array to speed up the\n",
        "calculation with separation the features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1EGYAWmlQmR"
      },
      "outputs": [],
      "source": [
        "# array = dataframe.values\n",
        "# X1 = array[:,0:8]\n",
        "# Y1 = array[:,8]\n",
        "# #print(X1)\n",
        "# #print(Y1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH0ILr_Dm_kz"
      },
      "source": [
        "The chi-square test is a statistical test of independence to measure the dependence of two variables, it is only applied to categorical data-classification- (for numerical data, we use R-squared-regression-).\n",
        "\n",
        "Suppose you have a target variable (label) and other characteristics (feature variables) that describe each data observation.\n",
        "\n",
        "Chi-squared consists of doing two statistical calculations between each feature variable and the target variable (label) and observing the existence of a relationship between the variables and the target.\n",
        "If the target variable is independent of the characteristic variable, we can reject the characteristic variable. If they are dependent, the characteristic variable is very important.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSTnfsNWm3h1"
      },
      "source": [
        "For this test, we will use the scikit-learn library which provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features, in this case, it is Chi-Squared.\n",
        "\n",
        "Mathematical details are described here:http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnkRv4UKnftk"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries first\n",
        "# from sklearn.feature_selection import SelectKBest\n",
        "# from sklearn.feature_selection import chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdd_SC45n1uu"
      },
      "outputs": [],
      "source": [
        "# # Feature extraction\n",
        "# test = SelectKBest(score_func=chi2, k=4)\n",
        "## chi2: Chi-squared stats of non-negative features for classification tasks.\n",
        "# fit = test.fit(X1, Y1)\n",
        "\n",
        "# fit.scores_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI14GriYgHrL"
      },
      "outputs": [],
      "source": [
        "# # Summarize scores\n",
        "# Listfeatures= ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "# print(Listfeatures)\n",
        "# print('================================================================================')\n",
        "# np.set_printoptions(precision=3)\n",
        "# print(fit.scores_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1uutdJigHrM"
      },
      "outputs": [],
      "source": [
        "# features = fit.transform(X1)\n",
        "# # Summarize selected features\n",
        "# print(features[0:5,:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-BBgeGhq1Ha"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "You can now see the scores for each attribute and the 4 selected attributes (those with the highest scores)are: test, plas, age, mass.\n",
        "\n",
        "These scores will further help you to determine the best features to train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zXOJaWFyE1q"
      },
      "source": [
        "<font color='blue' ><h3>\n",
        "IV.3. Data transformation </p><h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqAgXuccyE1q"
      },
      "source": [
        "## A.Categorical features\n",
        "\n",
        "Transforming categorical data is an essential step during data preprocessing. sklearn’s machine learning library require the input dataset to always have numeric values it does not support categorical data.\n",
        "\n",
        "It is necessary to convert categorical features to a numerical representation.\n",
        "\n",
        "Before you start transforming your data, it is important to figure out if the feature you’re working on is ordinal (as opposed to nominal). An ordinal feature is best described as a feature with  ordered categories.\n",
        "\n",
        "Once you know what type of categorical data you’re working on, you can pick a suiting transformation tool. In sklearn that will be a OrdinalEncoder or LabelEncoder for ordinal data, and a OneHotEncoder for nominal data.\n",
        "\n",
        "Let’s consider a simple example to demonstrate how both classes are working. Create a dataframe with four entries and three features: sex, blood type and education level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoDPqSURyE1q"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp8chUI8yE1q"
      },
      "outputs": [],
      "source": [
        "# X1 = pd.DataFrame(\n",
        "#     np.array(['M', 'O-', 'medium',\n",
        "#              'M', 'O-', 'high',\n",
        "#               'F', 'O+', 'high',\n",
        "#               'F', 'AB', 'low']\n",
        "#             )\n",
        "#               .reshape((4,3)))\n",
        "# X1.columns = ['sex', 'blood_type', 'edu_level']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ZaEmPfyE1q"
      },
      "outputs": [],
      "source": [
        "# X1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-jW925kyE1r"
      },
      "source": [
        "### 1.Ordinal Feature\n",
        "Looking at the dataframe, you should notice education level is the only ordinal feature (it can be ordered and the distance between the categories is not known). We’ll start with encoding this feature with the OrdinalEncoder class. Import the class and create a new instance. Then update the education level feature by fitting and transforming the feature to the encoder. The result should look as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InFEsUb6yE1r"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import OrdinalEncoder\n",
        "# encoder = OrdinalEncoder()\n",
        "# X1.edu_level = encoder.fit_transform(X1.edu_level.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMb2VonJyE1r"
      },
      "outputs": [],
      "source": [
        "# X1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM3YPOnUyE1r"
      },
      "source": [
        "Notice that we have a rather annoying issue here: our missing value is encoded as a separate class (3.0). Looking thoroughly at the documentation reveals that there is no solution for this issue yet. A good sign is that the sklearn developers are discussing the possibilities of implementing a suiting solution.\n",
        "Another problem is that the order of our data is not respected. This can luckily be solved by passing an ordered list of unique values for the feature to the categories parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SBGx9Y7yE1r"
      },
      "outputs": [],
      "source": [
        "# X = pd.DataFrame(\n",
        "#     np.array(['M', 'O-', 'medium',\n",
        "#              'M', 'O-', 'high',\n",
        "#               'F', 'O+', 'high',\n",
        "#               'F', 'AB', 'low']\n",
        "#             )\n",
        "#               .reshape((4,3)))\n",
        "\n",
        "\n",
        "# X.columns = ['sex', 'blood_type', 'edu_level']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ntdq_nFyE1r"
      },
      "outputs": [],
      "source": [
        "# encoder = OrdinalEncoder(categories=[[ 'low', 'medium', 'high']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59lih4obyE1r"
      },
      "outputs": [],
      "source": [
        "# X.edu_level = encoder.fit_transform(X.edu_level.values.reshape(-1, 1))\n",
        "\n",
        "# X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-qksyZryE1r"
      },
      "source": [
        "#### OrdinalEncoder vs LabelEncoder\n",
        "\n",
        "OrdinalEncoder is for 2D data with the shape (n_samples, n_features)\n",
        "\n",
        "LabelEncoder is for 1D data with the shape (n_samples,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8NLxOqyE1r"
      },
      "source": [
        "### 2. Nominal Features\n",
        "\n",
        "\n",
        "The most popular way to encode nominal features is one-hot-encoding. Essentially, each categorical feature with n categories is transformed into n binary features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9TphDhyE1r"
      },
      "source": [
        "Start with importing the OneHotEncoder class and creating a new instance with the output data type set to integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVgpCrrSyE1s"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# onehot = OneHotEncoder(dtype=np.int, sparse=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ok8aw8yyE1s"
      },
      "source": [
        "Then, fit and transform our two nominal categoricals. The output of this transformation will be a sparse matrix, which we will transform the matrix to an array (.toarray()) before we can pour it into a dataframe. Assign column names and the output is ready to be added to the other data (edu_level feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdR-eVyFyE1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# nominals = pd.DataFrame(\n",
        "#     onehot.fit_transform(X[['sex', 'blood_type']]).toarray(),\n",
        "#     columns=['F', 'M', 'AB','O+', 'O-'])\n",
        "# nominals['edu_level'] = X.edu_level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMsuHXGyE1s"
      },
      "source": [
        "Compare the output (nominals) to our original data to make sure everything came through the right way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9NdRuX9yE1s"
      },
      "outputs": [],
      "source": [
        "# nominals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gwrrQ-DyE1s"
      },
      "outputs": [],
      "source": [
        "# X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLfPEsryE1s"
      },
      "source": [
        "## B. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAqNGq1iyE1s"
      },
      "source": [
        "\n",
        "\n",
        "Feature scaling is a very important step in the preprocessing pipeline.\n",
        "\n",
        "Before applying any scaling operation it is very important to split your data into a train set and a test set. If you start scaling before, your training (and test) data might end up scaled around a mean value (see below) that is not actually the mean of the train or test data, and go past the whole reason why you're scaling in the first place.\n",
        "\n",
        "\n",
        "\n",
        "Standardization is a transformation that centers the data by removing the mean value of each feature and then scale it by dividing (non-constant) features by their standard deviation. After standardizing data the mean will be zero and the standard deviation one.\n",
        "Standardization can drastically improve the performance of models. For instance, many learning algorithms (such as Support Vector Machines) assume that all features are centered around zero and have variance in the same order.\n",
        "Depending on your needs and data, sklearn provides a bunch of scalers: StandardScaler, MinMaxScaler, MaxAbsScaler and RobustScaler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWfOnnJoyE1t"
      },
      "source": [
        "### Methode (1) Standardization (Standard Scaler)\n",
        "\n",
        "\n",
        "Sklearn its main scaler, the StandardScaler, uses a strict definition of standardization to standardize data. It purely centers the data by using the following formula,\n",
        "\n",
        "# x_scaled = (x - u) / s\n",
        "\n",
        "## where u is the mean and s is the standard deviation.\n",
        "\n",
        "Let’s take a look at our example to see this in practice.\n",
        "Import the StandardScaler class and create a new instance. Then, fit and transform the scaler to feature 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OtiqAGQyE1t"
      },
      "outputs": [],
      "source": [
        "# Y = pd.DataFrame(\n",
        "#     np.array([5,7,8, np.NaN, np.NaN, np.NaN, -5,\n",
        "#               0,25,999,1,-1, np.NaN, 0, np.NaN])\\\n",
        "#               .reshape((5,3)))\n",
        "# Y.columns = ['f1', 'f2', 'f3'] #feature1, feature 2, feature 3\n",
        "# Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoAFbqpQyE1t"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit_transform(Y.f3.values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twyXuhC5rQOk"
      },
      "source": [
        "\n",
        "\n",
        "### Method(2): Normalization:\n",
        "\n",
        "Data normalization is a fundamental component, It entails transforming the data, or turning the source data into another format that enables for successful data processing.\n",
        "\n",
        "The MinMaxScaler transforms features by scaling each feature to a given range. This range can be set by specifying the feature_range parameter (default at $(0,1)$).\n",
        "\n",
        "This scaler works better for cases where the distribution is not Gaussian or the standard deviation is very small. However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider another scaler.\n",
        "\n",
        "$$x\\_scaled = \\dfrac{(x-min(x))}{(max(x)–min(x))}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVtiFClbyE1t"
      },
      "source": [
        "#### Method 1:  Manual Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbHBzFgRrKLd"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # data frame containing the odometer reading (km) and the fuel economy (km/l) of second-hand cars\n",
        "# df_cars = pd.DataFrame([[120000, 11], [250000, 11.5], [175000, 15.8], [350000, 17], [400000, 10]],\n",
        "#                        columns=['odometer_reading', 'fuel_economy'])\n",
        "\n",
        "# df_cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILT_qmq4zYmO"
      },
      "outputs": [],
      "source": [
        "# df_cars.plot(kind='density',figsize=(13,5))\n",
        "# plt.title('All variables with different scales', fontsize=20)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krfiJuEO0gse"
      },
      "outputs": [],
      "source": [
        "# # apply the min-max scaling in Pandas using the .min() and .max() methods\n",
        "# def min_max_scaling(df):\n",
        "#     # copy the dataframe\n",
        "#     df_norm = df.copy()\n",
        "#     # apply min-max scaling\n",
        "#     for column in df_norm.columns:\n",
        "#         df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
        "\n",
        "#     return df_norm\n",
        "\n",
        "# # call the min_max_scaling function\n",
        "# df_cars_normalized = min_max_scaling(df_cars)\n",
        "\n",
        "# df_cars_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l5F1KKV0hT8"
      },
      "outputs": [],
      "source": [
        "# df_cars_normalized.plot(kind='density',figsize=(13,5))\n",
        "# plt.title('All variables with same scales', fontsize=20)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k22hPrOhyE1u"
      },
      "source": [
        "#### Method 2:  Using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHHVrVtByE1u"
      },
      "outputs": [],
      "source": [
        "# df_cars.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXYfB81JyE1u"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler(feature_range=(0,1))\n",
        "# df_cars_normalized2=scaler.fit_transform(df_cars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bpQrB5dyE1u"
      },
      "outputs": [],
      "source": [
        "# df_cars_normalized2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNzsNG-oyE1u"
      },
      "source": [
        "#####  <font color='purple' > When Should You Use Normalization and Standardization:\n",
        "                                                                                       \n",
        "Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n",
        "\n",
        "Standardization assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95vGklCWyE1u"
      },
      "source": [
        "<font color='blue' ><h3>\n",
        "IV.4.  Dimensionality reduction </p><h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQWbW5ffC53y"
      },
      "source": [
        "\n",
        "The dimensionality of a dataset is defined as the number of input variables or characteristics.\n",
        "\n",
        "\n",
        "Dimensionality reduction techniques are those that lower the number of variables in a dataset.\n",
        "\n",
        "\n",
        "For data visualization, high-dimensionality statistics and dimensionality reduction techniques are frequently used.\n",
        "Nonetheless, in applied machine learning, similar strategies can be utilized to reduce a classification or regression dataset in order to better train a predictive model.\n",
        "\n",
        "## PCA (Principal component Anlysis):\n",
        "Principal component Anlysis is a dimensionality reduction technique used to reduce the number of predictor variables in a dataset.\n",
        "\n",
        "\n",
        "PCA is a sort of unsupervised feature extraction in which original variables are integrated and reduced to their most important and descriptive components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yH2XGz2yE1u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHyTZ5MDyE1u"
      },
      "source": [
        "### Method(1): Manual Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUBbxWrYC5DP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from numpy import array\n",
        "# from numpy import mean\n",
        "# from numpy import cov\n",
        "# from numpy.linalg import eig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7JcJyVyyE1v"
      },
      "source": [
        "The required work is to reduce the dimension of the stat2 dataset using PCA in two ways:\n",
        "1. From scratch using manual computation\n",
        "2. Computing using \"pca\" command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vm8iIGHyE1v"
      },
      "source": [
        "## DataSet: stat2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-zWdKG9yE1v"
      },
      "outputs": [],
      "source": [
        "# Missing value imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cFYMvrAEqTP"
      },
      "source": [
        "### Step(1): Data centering and reduction\n",
        "\n",
        "The purpose of this step is to standardize the range of initial continuous variables so that each contributes equally to the analysis.\n",
        "Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.\n",
        "Once standardization is complete, all variables will be transformed to the same scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "terZah-dEnIy"
      },
      "outputs": [],
      "source": [
        "##Data centering and reduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bSVF-FzIOk0"
      },
      "source": [
        "### Step(2): COMPUTATION OF THE COVARANCE MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVGYew4XIOJ8"
      },
      "outputs": [],
      "source": [
        "# compute the covariance matrix of the centered matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQxV0QCIl2y"
      },
      "source": [
        "#### Step(3): Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principle components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "309l9en7Cuad"
      },
      "outputs": [],
      "source": [
        "# eigenvectors and eigenvalues of covariance matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q47cnnGJajQ"
      },
      "source": [
        "### Step(4): Projection of the initial data on the new axes (the principal components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUx0Pj4rJPW_"
      },
      "outputs": [],
      "source": [
        "# project data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd8PJMUOJuNe"
      },
      "source": [
        "## Method 2: Computation using \"PCA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8tNtIj5gHrh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Principal Component Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbgODnrRgHrh"
      },
      "outputs": [],
      "source": [
        "# create the PCA instance\n",
        "\n",
        "\n",
        "# fit on data: Apply PCA on data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wpTXQSHgHrj",
        "outputId": "06c3420d-ef17-48e7-e500-ca1df35d0d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================================\n"
          ]
        }
      ],
      "source": [
        "# access values and vectors/\n",
        "\n",
        "#Principal components: eigenvectors\n",
        "\n",
        "print(\"===================================================\")\n",
        "#Variance proportions associated with the axes: eigenvalues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9jy2CPOgHrj"
      },
      "outputs": [],
      "source": [
        "# transform data/ Projection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCthqxVRgHrk"
      },
      "outputs": [],
      "source": [
        "# PCA scree plot:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOfDqkwwgHrk",
        "outputId": "eba9f4d4-9068-4b5b-baf1-37848339a507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================================\n",
            "===================================================\n"
          ]
        }
      ],
      "source": [
        "# create the PCA instance\n",
        "\n",
        "# fit on data: Apply PCA on the data\n",
        "\n",
        "\n",
        "#Principal components: eigenvectors\n",
        "\n",
        "print(\"===================================================\")\n",
        "#Variance proportions associated with the axes: eigenvalues.\n",
        "\n",
        "print(\"===================================================\")\n",
        "# transform data/ Projection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH9NSpi0KDmY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}